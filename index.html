<!DOCTYPE html>
<html>
	<head>
		<title>CSE 455 Final Project</title>
        <link rel="stylesheet" type="text/css" href="/css/style.css">
		<link rel="shortcut icon" href="imgs/xray.jpg" type="image/jpg">  
	</head>
	<body>
		<nav>
    		<ul>
				<li><a href="https://github.com/oscar-perez1/oscar-perez1.github.io">Repository</a></li>
        		<li><a href="http://github.com/oscar-perez1/oscar-perez1.github.io/tree/main/code">Code</a></li>
	        	<li><a href="https://drive.google.com/drive/folders/1QaExut1A8IspwQLGP0fpDmzJq4IQPsnj?usp=sharing">Dataset</a></li>
				<li><a href="video link">Video</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
        		<h1>X-ray Abnormalities Classification</h1>
                    
                </p>
                    <h2>Problem Description</h2>
					<p>
						Radiologists are hugely important to diagnosing conditions from medical images,
						such as MRIs, CT scans, and X-ray scans. In our project, we aim to provide an
						extra perspective to X-ray scans using computer vision. We do this by providing
						a best guess for the classification for a given area of a chest x-ray. Our inspiration
						for this project came from this Kaggle competition: <a href="https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection">VinBigData Chest X-ray Abnormalities
						Detection | Kaggle</a>, but we wanted to focus on the classification of the certain abnormality regions.
						This gives us more time to consider the models currently used in object classification, trying transfer
						learning as well as building our own classification models.
					</p>

                    <h2>Previous Work</h2>
					<p>
						We used a number of different inspirations for our approach to abnormality classification.

						<h3>Pre-trained network (Inceptionv3)</h3>
						We experimented with utilizing some of the provided models that were pre-trained on the ImageNet dataset
						for our specific classification problem. One of the models utilized included Inceptionv3 loaded from the
						torchvision models subpackage. Adapting this to our problems involved rearranging the input images to be
						of proper dimensions, as the Inceptionv3 model required images to be of at least size 300x300, and modifying
						the final fully connected layer to have an output size of 14. After experimenting with hyperparameters, the
						following parameters resulted in the highest test accuracy of 71% with epochs=5, lr=0.001, decay=0.0005, batch_size=128.

						<h3>Pre-trained network (ResNet50)</h3>
						We used transfer learning on the pretrained model and changed the final layer to a fully connected linear
						layer with an output of 14 (per our classification problem). Data augmentations applied to this model
						included random flippings as well as random cropping. After playing with the hyperparameters and 
						applying data augmentation and simulated annealing, the following parameters resulted in the highest test
						accuracy of 75% with epochs=2, lr=0.001, decay=0.0005, batch_size=32.<br><br>

						Further tuning of the model seemed to overfit heavily resulting in a lower testing accuracy, and further training
						using a deeper network like resnet 152 did increase in a 1% increase in testing accuracy and 1% decrease in
						training accuracy. However, each epoch with the ResNet152 took about ~5 minutes which was much slower than
						ResNet50.

						<h3>Pre-trained network (AlexNet)</h3>
						We tried to use AlexNet which is not as deep as resnet with 8 layers (5 convolutional followed by max pooling layers).
						It was much faster to train with just 30s/ epoch. However, we found that we have to start with a much lower learning
						rate for this i.e. 0.0000001 otherwise the loss would go to NaN. We were able to achieve a test and train accuracy of
						~0.64 before the model started overfitting and the test accuracy went down. 
						
						<h3>Pre-trained networkk (VGG16)</h3>
						Using this model which is 16 layer deep we found it to be slightly better than AlexNet. This was expected since it does
						perform better on the ImageNet dataset compared to AlexNet). The train time was slightly slower with 150s/epoch but we
						were able to achieve a train and test accuracy of ~68%.
					</p>

                    <h2>Approach</h2>
						<h3>Preprocessing the data for classification</h3>
						<p>
							As mentioned in the Dataset tab, our initial dataset contains full dicom X-ray images
							chests. The training set is labeled by radiologists, specifying regions of each X-ray
							in which abnormalities appear. To create our dataset for classification, we recreate
							the training set by going through each image and creating a new image of the cropped
							abnormalities, making sure to resize the image to be 64x64 for the input of our models.
							After this, we split this training set into labeled training, validation, and test sets.
							The code to parse the dicom data is in ConvDataPrep.ipynb and the code to split the
							training data into multiple sets is in SplitTrainTest.ipynb.
						</p>

						<h3>Building our own network for classification</h3>
						<p>
							We approached building our own network by using the demo from Professor Redmon’s lecture
							on Convolutional Neural Networks. We started by reshaping each of the models to fit our
							data, as the cropped images of abnormalities we used were of size 64x64 whereas the Cifar-10
							images were of size 32x32. Additionally, we needed to change the output layer to be of size
							14, as we have 14 labels as opposed to the 10 labels of Cifar-10. We made some new modifications
							to the convolutional models, modifying the number of input channels for the first layer to 1, as
							the X-rays are black and white, and changing the final layer to have the appropriate amount of
							inputs and outputs, as the number of features is increased from the first layer being of dimension
							64x64 and we are classifying 14 abnormalities. We did this for all of the demo neural networks, as
							well as implementing random augmentation on the training set, and trained with annealing.
							<br><br>
							Additionally, we worked on building our own convolutional neural network, increasing the number of
							layers and taking inspiration from how VGG-19 is developed. We noticed from the demonstrational CNN’s,
							each convolutional layer has a stride of two, whereas VGG-19 only increases the stride every few layers.
							We then created a number of different CNNs with more layers, creating a deeper neural network.
						</p>

					<h3>Models we implemented</h3>
					<ul>
						<li>CustomNet5</li>
							<ul>
								<li>5 Convolutional Layers with batch normalization and a linear layer</li>
								<li>Structure:</li>
									<ul>
										<li>Conv2d(1, 16, 5, stride=2, padding=1)</li>
										<li>BatchNorm2d(16)</li>
										<li>Conv2d(16, 32, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(32)</li>
										<li>Conv2d(32, 64, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(64)</li>
										<li>Conv2d(64, 128, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(128)</li>
										<li>Linear(8192, 14)</li>
									</ul>
							</ul>
						<li>CustomNet8</li>
							<ul>
								<li>7 Convolutional Layers with batch normalization and a linear layer</li>
								<li>Structure:</li>
									<ul>
										<li>Conv2d(1, 16, 5, stride=2, padding=1)</li>
										<li>Conv2d(16, 16, 5, stride=1, padding=1)</li>
										<li>BatchNorm2d(16)</li>
										<li>Conv2d(16, 32, 3, stride=2, padding=1)</li>
										<li>Conv2d(32, 32, 3, stride=1, padding=1)</li>
										<li>BatchNorm2d(32)</li>
										<li>Conv2d(32, 64, 3, stride=2, padding=1)</li>
										<li>Conv2d(64, 64, 3, stride=1, padding=1)</li>
										<li>BatchNorm2d(64)</li>
										<li>Conv2d(64, 128, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(128)</li>
										<li>Linear(8192, 14)</li>
									</ul>
							</ul>
						<li>CustomNet10</li>
							<ul>
								<li>7 Convolutional Layers with batch normalization and 3 linear layers</li>
								<li>Structure:</li>
									<ul>
										<li>Conv2d(1, 16, 5, stride=2, padding=1)</li>
										<li>Conv2d(16, 16, 5, stride=1, padding=1)</li>
										<li>BatchNorm2d(16)</li>
										<li>Conv2d(16, 32, 3, stride=2, padding=1)</li>
										<li>Conv2d(16, 32, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(32)</li>
										<li>Conv2d(32, 64, 3, stride=2, padding=1)</li>
										<li>Conv2d(32, 64, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(64)</li>
										<li>Conv2d(64, 64, 3, stride=1, padding=1)</li>
										<li>Conv2d(64, 128, 3, stride=2, padding=1)</li>
										<li>BatchNorm2d(128)</li>
										<li>Linear(8192, 8192)</li>
										<li>Linear(8192, 8192)</li>
										<li>Linear(8192, 14)</li>
									</ul>
							</ul>
					</ul>

					<h3>Random search for hyperparameters</h3>
					<p>
						Instead of using a grid search algorithm to tune hyper parameters, we picked random values for each
						hyper parameter at each iteration. We wanted to tune batch size, starting learning rate, momentum,
						and decay. Using grid search to iterate through all possible combinations would have been prohibitively
						time consuming since the search space increases dramatically with each additional variable. Thus, we
						opted to use a random search algorithm instead. We also found a <a href="https://www.jmlr.org/papers/v13/bergstra12a.html">
						study that showed that a random search algorithm was more efficient than grid search</a>.
					</p>

                    <h2>Datasets</h2>
						<p>
							We found our dataset from this kaggle competition: <a href="https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection">VinBigData
							Chest X-ray Abnormalities Detection | Kaggle</a>. This dataset provided 15000 training samples, in the form of dicom images. Each image was
							classified by a radiologist, From this dataset, we created a dataset for classification, as the original dataset is meant for abnormality
							detection rather than classification. To get the data for classification, we used two steps: cropping the X-rays into the abnormality region
							as defined by the radiologists, and splitting the newly cropped images into training, validation, and testing.<br><br>

							To start, we get images like this: <br><br>

							<img src="imgs/FullXray.png" alt="Default full X-ray" width="500" height="500"> <br><br>

							This was found to have 5 potential abnormalities, diagnosed and located by multiple radiologists. Given the diagnoses, we produced the following
							images with the corresponding labels: <br>

							<h3>Lung Opacity</h3>
							<img src="imgs/LungOpacity1.png" alt="Lung opacity region" width="300" height="300"> <br><br>
							<h3>Nodule/Mass</h3>
							<img src="imgs/NoduleMass.png" alt="Nodule mass region" width="300" height="300"> <br><br>
							<h3>Infiltration</h3>
							<img src="imgs/Infiltration.png" alt="Infiltration region" width="300" height="300"> <br><br>
							<h3>Lung Opacity</h3>
							<img src="imgs/LungOpacity2.png" alt="Lung opacity region" width="300" height="300"> <br><br>
							<h3>Consolidation</h3>
							<img src="imgs/Consolidation.png" alt="Consolidation region" width="300" height="300"> <br><br>

							The dataset can be accessed via the tab at the top of the page (a Google Drive folder utilized during
							work on the project via Google Colab).
						</p>

                    <h2>Results</h2>
					<p>
						Below is a table of the results of the predictions performed by each model.<br><br>

						<img src="/imgs/results-table.png" alt="Table of results for models" width="729" height="697">
					</p>

                    <h2>Discussion</h2>
                        <h3>Problems Encountered</h3>
						<p>
							The first major problem when we tackled this project was getting a sufficient dataset to tackle the problem we wanted to face. This involved
							processing the data we got from the Kaggle to create a new dataset of chest abnormalities. Processing and reading the dicom images was an
							issue to overcome but we found some useful libraries online to help with reading them, such as pydicom. Another problem was the runtime of
							large pretrained was too high to tune hyperparameters. This happened specifically for ResNet-152 so we found the hyperparameters that worked
							best with ResNet-50 to train ResNet-152. We also had to ensure that the dataset and images we were working with were formatted as inputs that
							the pre-trained networks could handle, as some of them required resizing and restructuing of the data passed in during training.					
						</p>

                        <h3>Next Steps</h3>
						<p>
							Since the dataset we based our project on was meant for abnormality detection rather than classification, the next steps we would take would
							be to expand on our classifiers and use them for abnormality detection.
						</p>

                        <h3>Approach Uniqueness</h3>
						<p>
							We decided to try multiple approaches to classification, both building a model from our own design and by doing transfer learning. In most
							cases, teams would likely use transfer learning, as there are a number of highly performant models. We found it beneficial to do both,
							learning from our own designs of a CNN for specifically X-rays, while also seeing how well image classifiers trained for more conventional
							images, worked on X-rays.
						</p>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
		</footer>
	</body>
</html>